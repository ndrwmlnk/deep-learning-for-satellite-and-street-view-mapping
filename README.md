# 📖 Deep Learning for Satellite and Street View Mapping: A Survey

---

📄 [**Cross-view Transformers for real-time Map-view Semantic Segmentation**](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Cross-View_Transformers_for_Real-Time_Map-View_Semantic_Segmentation_CVPR_2022_paper.pdf)  
🌐 https://github.com/bradyz/cross_view_transformers

<img src='https://raw.githubusercontent.com/bradyz/cross_view_transformers/master/docs/assets/predictions.gif' width="50%">
<img src='https://raw.githubusercontent.com/bradyz/cross_view_transformers/master/docs/assets/map.gif' width="50%">

---

📄 [**NEAT: Neural Attention Fields for End-to-End Autonomous Driving**](https://openaccess.thecvf.com/content/ICCV2021/html/Chitta_NEAT_Neural_Attention_Fields_for_End-to-End_Autonomous_Driving_ICCV_2021_paper.html)  
🌐 https://github.com/autonomousvision/neat  
📺 https://youtu.be/gtO-ghjKkRs

<img src='https://github.com/autonomousvision/neat/raw/main/neat/assets/neat_clip.GIF'>

---

📄 [**Driving among Flatmobiles: Bird-Eye-View occupancy grids from a monocular camera for holistic trajectory planning**](https://arxiv.org/abs/2008.04047)  
📺 https://www.youtube.com/watch?v=ys-LRewgNYs  

|<img src='/imgs/Driving among Flatmobiles.jpg'>
|:--:|
| Figure 6: Bird-eye-view qualitative results for the first stage of the network. The blue part of the predicted masks corresponds to the limits of the camera’s field of view. GT stands for Ground Truth. |![image](https://user-images.githubusercontent.com/22514465/180946369-a6d475d1-2a21-43e2-bcb5-4445f4e98eab.jpeg)


---

📄 [**Monocular Semantic Occupancy Grid Mapping with Convolutional Variational Encoder-Decoder Networks**](https://arxiv.org/abs/1804.02176)

|<img src='/imgs/Monocular Semantic Occupancy.jpg'>
|:--:|
| Fig. 1. An illustration of the proposed variational encoder-decoder approach. From a single front-view RGB image, our system can predict a 2-D top-view semantic-metric occupancy grid map. |

---

📄 [**Orthographic Feature Transform for Monocular 3D Object Detection**](https://arxiv.org/abs/1811.08188)

|<img src='/imgs/Orthographic Feature Transform.jpg'>
|:--:|
| Figure 3. Architecture overview. A front-end ResNet feature extractor generates image-based features, which are mapped to an orthographic representation via our proposed orthographic feature transform. The topdown network processes these features in the birds-eye-view space and at each location on the ground plane predicts a confidence score S, a position offset ∆pos, a dimension offset ∆dim and an angle vector ∆ang. |

---

📄 [**Monocular 3D Vehicle Detection Using Uncalibrated Traffic Cameras through Homography**](https://arxiv.org/abs/2103.15293)  
🌐 https://github.com/minghanz/trafcam_3d

|<img src='/imgs/Monocular 3D Vehicle Detection Using Uncalibrated Traffic Cameras through Homography.jpg'>
|:--:|
| Fig. 1: The 3D vehicle detection problem is transformed to a 2D detection problem in warped bird’s eye view (BEV) images. The orange lines attached to each orange boxes are tails, defined in Sec. III-C.1 and Fig. 5, which are regressed by the network to better handle distortions in BEV images. |

---

📄 [**Unsupervised Learning of Depth and Ego-Motion from Video**](https://paperswithcode.com/paper/unsupervised-learning-of-depth-and-ego-motion-1)
🌐 https://github.com/tinghuiz/SfMLearner
📺 https://youtu.be/HWu39YkGKvI

<img src='https://github.com/tinghuiz/SfMLearner/raw/master/misc/cityscapes_sample_results.gif'>

---

📄 [**Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations**](https://arxiv.org/abs/2111.13152)  
🌐 https://srt-paper.github.io

<img src='https://srt-paper.github.io/data/streetview/input1.png'>

---

📄 [**Look Outside the Room: Synthesizing A Consistent Long-Term 3D Scene Video from A Single Image**](https://arxiv.org/abs/2203.09457)  
🌐 https://xrenaa.github.io/look-outside-room/  
📺 https://youtu.be/eSftXilv21s  

<img src='https://xrenaa.github.io/look-outside-room/static/images/method.png' width="70%">

---

📄 [**MP3: A Unified Model to Map, Perceive, Predict and Plan**](https://arxiv.org/abs/2101.06806)  

|<img src='/imgs/MP3 - A Unified Model to Map, Perceive, Predict and Plan.jpg'>
|:--:|
| Figure 2: MP3 predicts probabilistic scene representations that are leveraged in motion planning as interpretable cost functions. |

---

📄 [**Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D**](https://arxiv.org/abs/2008.05711)  
🌐 https://nv-tlabs.github.io/lift-splat-shoot/  
🌐 https://github.com/nv-tlabs/lift-splat-shoot  
📺 https://youtu.be/oL5ISk6BnDE

<img src='/imgs/Lift, Splat, Shoot.gif'>

---

## Other papers

📄 [**awesome-lane-detection**](https://github.com/amusi/awesome-lane-detection)

📄 [**TransformerFusion: Monocular RGB SceneReconstruction using Transformers**](https://arxiv.org/abs/2107.02191)  
📺 https://www.youtube.com/watch?v=ys-LRewgNYs  

📄 [**Calibration of Inverse Perspective Mapping from Different Road Surface Images**](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9612531)

