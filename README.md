# ðŸ“– Deep Learning for Satellite and Street View Mapping: A Survey

---

ðŸ“„ [**Driving among Flatmobiles: Bird-Eye-View occupancy grids from a monocular camera for holistic trajectory planning**](https://arxiv.org/abs/2008.04047)  
ðŸ“º Video: https://www.youtube.com/watch?v=ys-LRewgNYs  

|<img src='/imgs/Driving among Flatmobiles.jpg'>
|:--:|
| Figure 6: Bird-eye-view qualitative results for the first stage of the network. The blue part of the predicted masks corresponds to the limits of the cameraâ€™s field of view. GT stands for Ground Truth. |

---

ðŸ“„ [**Monocular Semantic Occupancy Grid Mapping with Convolutional Variational Encoder-Decoder Networks**](https://arxiv.org/abs/1804.02176)

|<img src='/imgs/Monocular Semantic Occupancy.jpg'>
|:--:|
| Fig. 1. An illustration of the proposed variational encoder-decoder approach. From a single front-view RGB image, our system can predict a 2-D top-view semantic-metric occupancy grid map. |

---

ðŸ“„ [**Orthographic Feature Transform for Monocular 3D Object Detection**](https://arxiv.org/abs/1811.08188)

|<img src='/imgs/Orthographic Feature Transform.jpg'>
|:--:|
| Figure 3. Architecture overview. A front-end ResNet feature extractor generates image-based features, which are mapped to an orthographic representation via our proposed orthographic feature transform. The topdown network processes these features in the birds-eye-view space and at each location on the ground plane predicts a confidence score S, a position offset âˆ†pos, a dimension offset âˆ†dim and an angle vector âˆ†ang. |

---

ðŸ“„ [**Unsupervised Learning of Depth and Ego-Motion from Video**](https://paperswithcode.com/paper/unsupervised-learning-of-depth-and-ego-motion-1)

ðŸ“„ [**Scene Representation Transformer**](https://srt-paper.github.io)

ðŸ“„ [**awesome-lane-detection**](https://github.com/amusi/awesome-lane-detection)

---
Tarek  

ðŸ“„ [**NEAT: Neural Attention Fields for End-to-End Autonomous Driving**](https://openaccess.thecvf.com/content/ICCV2021/html/Chitta_NEAT_Neural_Attention_Fields_for_End-to-End_Autonomous_Driving_ICCV_2021_paper.html)

ðŸ“„ [**Monocular 3D Vehicle Detection Using Uncalibrated Traffic Cameras through Homography**](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636384)

ðŸ“„ [**Cross-view Transformers for real-time Map-view Semantic Segmentation**](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Cross-View_Transformers_for_Real-Time_Map-View_Semantic_Segmentation_CVPR_2022_paper.pdf)

---
Felix

ðŸ“„  
ðŸ“„  
ðŸ“„  
ðŸ“„  

