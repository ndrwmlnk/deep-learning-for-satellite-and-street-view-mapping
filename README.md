# ğŸ“– Deep Learning for Satellite and Street View Mapping: A Survey

---

ğŸ“„ [**Cross-view Transformers for real-time Map-view Semantic Segmentation**](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Cross-View_Transformers_for_Real-Time_Map-View_Semantic_Segmentation_CVPR_2022_paper.pdf)  
âœ… Code implementation available  
ğŸŒ https://github.com/bradyz/cross_view_transformers  

<img src='https://raw.githubusercontent.com/bradyz/cross_view_transformers/master/docs/assets/predictions.gif' width="50%">
<img src='https://raw.githubusercontent.com/bradyz/cross_view_transformers/master/docs/assets/map.gif' width="50%">

---

ğŸ“„ [**NEAT: Neural Attention Fields for End-to-End Autonomous Driving**](https://openaccess.thecvf.com/content/ICCV2021/html/Chitta_NEAT_Neural_Attention_Fields_for_End-to-End_Autonomous_Driving_ICCV_2021_paper.html)  
ğŸŒ https://github.com/autonomousvision/neat  
ğŸ“º https://youtu.be/gtO-ghjKkRs

<img src='https://github.com/autonomousvision/neat/raw/main/neat/assets/neat_clip.GIF'>

---

ğŸ“„ [**Driving among Flatmobiles: Bird-Eye-View occupancy grids from a monocular camera for holistic trajectory planning**](https://arxiv.org/abs/2008.04047)  
ğŸ“º https://www.youtube.com/watch?v=ys-LRewgNYs  

|<img src='/imgs/Driving among Flatmobiles.jpg'>
|:--:|
| Figure 6: Bird-eye-view qualitative results for the first stage of the network. The blue part of the predicted masks corresponds to the limits of the cameraâ€™s field of view. GT stands for Ground Truth. |![image](https://user-images.githubusercontent.com/22514465/180946369-a6d475d1-2a21-43e2-bcb5-4445f4e98eab.jpeg)


---

ğŸ“„ [**Monocular Semantic Occupancy Grid Mapping with Convolutional Variational Encoder-Decoder Networks**](https://arxiv.org/abs/1804.02176)

|<img src='/imgs/Monocular Semantic Occupancy.jpg'>
|:--:|
| Fig. 1. An illustration of the proposed variational encoder-decoder approach. From a single front-view RGB image, our system can predict a 2-D top-view semantic-metric occupancy grid map. |

---

ğŸ“„ [**Orthographic Feature Transform for Monocular 3D Object Detection**](https://arxiv.org/abs/1811.08188)

|<img src='/imgs/Orthographic Feature Transform.jpg'>
|:--:|
| Figure 3. Architecture overview. A front-end ResNet feature extractor generates image-based features, which are mapped to an orthographic representation via our proposed orthographic feature transform. The topdown network processes these features in the birds-eye-view space and at each location on the ground plane predicts a confidence score S, a position offset âˆ†pos, a dimension offset âˆ†dim and an angle vector âˆ†ang. |

---

ğŸ“„ [**Monocular 3D Vehicle Detection Using Uncalibrated Traffic Cameras through Homography**](https://arxiv.org/abs/2103.15293)  
ğŸŒ https://github.com/minghanz/trafcam_3d

|<img src='/imgs/Monocular 3D Vehicle Detection Using Uncalibrated Traffic Cameras through Homography.jpg'>
|:--:|
| Fig. 1: The 3D vehicle detection problem is transformed to a 2D detection problem in warped birdâ€™s eye view (BEV) images. The orange lines attached to each orange boxes are tails, defined in Sec. III-C.1 and Fig. 5, which are regressed by the network to better handle distortions in BEV images. |

---

ğŸ“„ [**FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras**](https://arxiv.org/abs/2008.04047)  
âœ… Code implementation available  
ğŸŒ https://wayve.ai/blog/fiery-future-instance-prediction-birds-eye-view/  
ğŸŒ https://github.com/wayveai/fiery  

<img src='https://cdn.sanity.io/images/rmgve84j/production/a12b28effdf77bde0c27c22d01479a57ed972bfc-924x717.gif' width="70%">

---

ğŸ“„ [**Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations**](https://arxiv.org/abs/2111.13152)  
âœ… Code implementation available  
ğŸŒ https://srt-paper.github.io  
ğŸŒ https://github.com/stelzner/srt  

<img src='https://srt-paper.github.io/data/streetview/input1.png'>

---

ğŸ“„ [**Look Outside the Room: Synthesizing A Consistent Long-Term 3D Scene Video from A Single Image**](https://arxiv.org/abs/2203.09457)  
ğŸŒ https://xrenaa.github.io/look-outside-room/  
ğŸ“º https://youtu.be/eSftXilv21s  

<img src='https://xrenaa.github.io/look-outside-room/static/images/method.png' width="70%">

---

ğŸ“„ [**MP3: A Unified Model to Map, Perceive, Predict and Plan**](https://arxiv.org/abs/2101.06806)  
â›” No code implementation  
ğŸŒ https://www.cs.toronto.edu/~sergio/publication/mp3/

|<img src='/imgs/MP3 - A Unified Model to Map, Perceive, Predict and Plan.jpg'>
|:--:|
| Figure 2: MP3 predicts probabilistic scene representations that are leveraged in motion planning as interpretable cost functions. |

---

ğŸ“„ [**Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D**](https://arxiv.org/abs/2008.05711)  
ğŸŒ https://nv-tlabs.github.io/lift-splat-shoot/  
ğŸŒ https://github.com/nv-tlabs/lift-splat-shoot  
ğŸ“º https://youtu.be/oL5ISk6BnDE

<img src='/imgs/Lift, Splat, Shoot.gif'>

---

ğŸ“„ [**VectorMapNet: End-to-end Vectorized HD Map Learning**](https://arxiv.org/abs/2206.08920)  
ğŸŒ https://tsinghua-mars-lab.github.io/vectormapnet/  
ğŸŒ https://github.com/Mrmoore98/VectorMapNet_code  

<img src='https://tsinghua-mars-lab.github.io/vectormapnet/images/VectorMapNet_Pipelinex2.gif' width="70%">

---

ğŸ“„ [**Trans4Map: Revisiting Holistic Top-down Mapping from Egocentric Images to Allocentric Semantics with Vision Transformers**](https://arxiv.org/abs/2207.06205)  
ğŸŒ https://paperswithcode.com/paper/trans4map-revisiting-holistic-top-down  
ğŸŒ https://github.com/jamycheung/trans4map  

<img src='https://github.com/jamycheung/Trans4Map/raw/main/fig_trans4map.png' width="70%">

---

ğŸ“„ [**LaRa: Latents and Rays for Multi-Camera Bird's-Eye-View Semantic Segmentation**](https://arxiv.org/abs/2206.13294)  

<img src='/imgs/LaRa.jpg'>

---

ğŸ“„ [**BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation**](https://arxiv.org/abs/2205.13542)  
ğŸŒ https://bevfusion.mit.edu  
ğŸŒ https://github.com/mit-han-lab/bevfusion  

<img src='https://github.com/mit-han-lab/bevfusion/raw/main/assets/demo.gif'>

---

ğŸ“„ [**ViT-BEVSeg: A Hierarchical Transformer Network for Monocular Birds-Eye-View Segmentation**](https://arxiv.org/abs/2205.15667)  
ğŸŒ https://github.com/robotvisionmu/ViT-BEVSeg  

<img src='/imgs/ViT-BEVSeg.jpg'>

---

ğŸ“„ [**Efficient and Robust 2D-to-BEV Representation Learning via Geometry-guided Kernel Transformer**](https://arxiv.org/abs/2206.04584)  
ğŸŒ https://github.com/hustvl/GKT  

<img src='/imgs/2D-to-BEV.jpg'>

---

ğŸ“„ [**Unsupervised Learning of Depth and Ego-Motion from Video**](https://paperswithcode.com/paper/unsupervised-learning-of-depth-and-ego-motion-1)  
ğŸŒ https://github.com/tinghuiz/SfMLearner  
ğŸ“º https://youtu.be/HWu39YkGKvI  

<img src='https://github.com/tinghuiz/SfMLearner/raw/master/misc/cityscapes_sample_results.gif'>

---

## Datasets

ğŸ“„ [**Mapillary**](https://arxiv.org/abs/1909.04422)  
ğŸŒ https://research.mapillary.com  
ğŸŒ https://www.mapillary.com/datasets  
ğŸ“º https://www.youtube.com/c/Mapillary  

ğŸ“„ [**nuScenes**](https://arxiv.org/abs/1903.11027)  
ğŸŒ https://nuscenes.org/nuscenes  
ğŸ“º https://youtu.be/4gkyUWSZUkg

---

## Diffusion Modeling

[**Flexible Diffusion Modeling of Long Videos**](https://arxiv.org/abs/2205.11495)  
ğŸŒ https://plai.cs.ubc.ca/2022/05/20/flexible-diffusion-modeling-of-long-videos/

<img src='/imgs/Flexible Diffusion Modeling of Long Videos.gif'>

---

## Other workds

ğŸŒ [**Map Track â€“ CARLA**](https://leaderboard.carla.org/leaderboard/)  

ğŸŒ [**paperswithcode.com: Bird's-Eye View Semantic Segmentation**](https://paperswithcode.com/task/bird-s-eye-view-semantic-segmentation/codeless)  

ğŸ“„ [**awesome-lane-detection**](https://github.com/amusi/awesome-lane-detection)

ğŸ“„ [**TransformerFusion: Monocular RGB SceneReconstruction using Transformers**](https://arxiv.org/abs/2107.02191)  
ğŸ“º https://www.youtube.com/watch?v=ys-LRewgNYs  

ğŸ“„ [**Calibration of Inverse Perspective Mapping from Different Road Surface Images**](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9612531)

