# 📖 Deep Learning for Satellite and Street View Mapping: A Survey

---

📄 [**Cross-view Transformers for real-time Map-view Semantic Segmentation**](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Cross-View_Transformers_for_Real-Time_Map-View_Semantic_Segmentation_CVPR_2022_paper.pdf)  
✅ Code implementation available  
🌐 https://github.com/bradyz/cross_view_transformers  

<img src='https://raw.githubusercontent.com/bradyz/cross_view_transformers/master/docs/assets/predictions.gif' width="50%">
<img src='https://raw.githubusercontent.com/bradyz/cross_view_transformers/master/docs/assets/map.gif' width="50%">

---

📄 [**NEAT: Neural Attention Fields for End-to-End Autonomous Driving**](https://openaccess.thecvf.com/content/ICCV2021/html/Chitta_NEAT_Neural_Attention_Fields_for_End-to-End_Autonomous_Driving_ICCV_2021_paper.html)  
🌐 https://github.com/autonomousvision/neat  
📺 https://youtu.be/gtO-ghjKkRs

<img src='https://github.com/autonomousvision/neat/raw/main/neat/assets/neat_clip.GIF'>

---

📄 [**Driving among Flatmobiles: Bird-Eye-View occupancy grids from a monocular camera for holistic trajectory planning**](https://arxiv.org/abs/2008.04047)  
📺 https://www.youtube.com/watch?v=ys-LRewgNYs  

|<img src='/imgs/Driving among Flatmobiles.jpg'>
|:--:|
| Figure 6: Bird-eye-view qualitative results for the first stage of the network. The blue part of the predicted masks corresponds to the limits of the camera’s field of view. GT stands for Ground Truth. |![image](https://user-images.githubusercontent.com/22514465/180946369-a6d475d1-2a21-43e2-bcb5-4445f4e98eab.jpeg)


---

📄 [**Monocular Semantic Occupancy Grid Mapping with Convolutional Variational Encoder-Decoder Networks**](https://arxiv.org/abs/1804.02176)

|<img src='/imgs/Monocular Semantic Occupancy.jpg'>
|:--:|
| Fig. 1. An illustration of the proposed variational encoder-decoder approach. From a single front-view RGB image, our system can predict a 2-D top-view semantic-metric occupancy grid map. |

---

📄 [**Orthographic Feature Transform for Monocular 3D Object Detection**](https://arxiv.org/abs/1811.08188)

|<img src='/imgs/Orthographic Feature Transform.jpg'>
|:--:|
| Figure 3. Architecture overview. A front-end ResNet feature extractor generates image-based features, which are mapped to an orthographic representation via our proposed orthographic feature transform. The topdown network processes these features in the birds-eye-view space and at each location on the ground plane predicts a confidence score S, a position offset ∆pos, a dimension offset ∆dim and an angle vector ∆ang. |

---

📄 [**Monocular 3D Vehicle Detection Using Uncalibrated Traffic Cameras through Homography**](https://arxiv.org/abs/2103.15293)  
🌐 https://github.com/minghanz/trafcam_3d

|<img src='/imgs/Monocular 3D Vehicle Detection Using Uncalibrated Traffic Cameras through Homography.jpg'>
|:--:|
| Fig. 1: The 3D vehicle detection problem is transformed to a 2D detection problem in warped bird’s eye view (BEV) images. The orange lines attached to each orange boxes are tails, defined in Sec. III-C.1 and Fig. 5, which are regressed by the network to better handle distortions in BEV images. |

---

📄 [**FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras**](https://arxiv.org/abs/2008.04047)  
✅ Code implementation available  
🌐 https://wayve.ai/blog/fiery-future-instance-prediction-birds-eye-view/  
🌐 https://github.com/wayveai/fiery  

<img src='https://cdn.sanity.io/images/rmgve84j/production/a12b28effdf77bde0c27c22d01479a57ed972bfc-924x717.gif' width="70%">

---

📄 [**Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations**](https://arxiv.org/abs/2111.13152)  
✅ Code implementation available  
🌐 https://srt-paper.github.io  
🌐 https://github.com/stelzner/srt  

<img src='https://srt-paper.github.io/data/streetview/input1.png'>

---

📄 [**Look Outside the Room: Synthesizing A Consistent Long-Term 3D Scene Video from A Single Image**](https://arxiv.org/abs/2203.09457)  
🌐 https://xrenaa.github.io/look-outside-room/  
📺 https://youtu.be/eSftXilv21s  

<img src='https://xrenaa.github.io/look-outside-room/static/images/method.png' width="70%">

---

📄 [**MP3: A Unified Model to Map, Perceive, Predict and Plan**](https://arxiv.org/abs/2101.06806)  
⛔ No code implementation  
🌐 https://www.cs.toronto.edu/~sergio/publication/mp3/

|<img src='/imgs/MP3 - A Unified Model to Map, Perceive, Predict and Plan.jpg'>
|:--:|
| Figure 2: MP3 predicts probabilistic scene representations that are leveraged in motion planning as interpretable cost functions. |

---

📄 [**Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D**](https://arxiv.org/abs/2008.05711)  
🌐 https://nv-tlabs.github.io/lift-splat-shoot/  
🌐 https://github.com/nv-tlabs/lift-splat-shoot  
📺 https://youtu.be/oL5ISk6BnDE

<img src='/imgs/Lift, Splat, Shoot.gif'>

---

📄 [**VectorMapNet: End-to-end Vectorized HD Map Learning**](https://arxiv.org/abs/2206.08920)  
🌐 https://tsinghua-mars-lab.github.io/vectormapnet/  
🌐 https://github.com/Mrmoore98/VectorMapNet_code  

<img src='https://tsinghua-mars-lab.github.io/vectormapnet/images/VectorMapNet_Pipelinex2.gif' width="70%">

---

📄 [**Trans4Map: Revisiting Holistic Top-down Mapping from Egocentric Images to Allocentric Semantics with Vision Transformers**](https://arxiv.org/abs/2207.06205)  
🌐 https://paperswithcode.com/paper/trans4map-revisiting-holistic-top-down  
🌐 https://github.com/jamycheung/trans4map  

<img src='https://github.com/jamycheung/Trans4Map/raw/main/fig_trans4map.png' width="70%">

---

📄 [**LaRa: Latents and Rays for Multi-Camera Bird's-Eye-View Semantic Segmentation**](https://arxiv.org/abs/2206.13294)  

<img src='/imgs/LaRa.jpg'>

---

📄 [**BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation**](https://arxiv.org/abs/2205.13542)  
🌐 https://bevfusion.mit.edu  
🌐 https://github.com/mit-han-lab/bevfusion  

<img src='https://github.com/mit-han-lab/bevfusion/raw/main/assets/demo.gif'>

---

📄 [**ViT-BEVSeg: A Hierarchical Transformer Network for Monocular Birds-Eye-View Segmentation**](https://arxiv.org/abs/2205.15667)  
🌐 https://github.com/robotvisionmu/ViT-BEVSeg  

<img src='/imgs/ViT-BEVSeg.jpg'>

---

📄 [**Efficient and Robust 2D-to-BEV Representation Learning via Geometry-guided Kernel Transformer**](https://arxiv.org/abs/2206.04584)  
🌐 https://github.com/hustvl/GKT  

<img src='/imgs/2D-to-BEV.jpg'>

---

📄 [**Unsupervised Learning of Depth and Ego-Motion from Video**](https://paperswithcode.com/paper/unsupervised-learning-of-depth-and-ego-motion-1)  
🌐 https://github.com/tinghuiz/SfMLearner  
📺 https://youtu.be/HWu39YkGKvI  

<img src='https://github.com/tinghuiz/SfMLearner/raw/master/misc/cityscapes_sample_results.gif'>

---

## Datasets

📄 [**Mapillary**](https://arxiv.org/abs/1909.04422)  
🌐 https://research.mapillary.com  
🌐 https://www.mapillary.com/datasets  
📺 https://www.youtube.com/c/Mapillary  

📄 [**nuScenes**](https://arxiv.org/abs/1903.11027)  
🌐 https://nuscenes.org/nuscenes  
📺 https://youtu.be/4gkyUWSZUkg

---

## Diffusion Modeling

[**Flexible Diffusion Modeling of Long Videos**](https://arxiv.org/abs/2205.11495)  
🌐 https://plai.cs.ubc.ca/2022/05/20/flexible-diffusion-modeling-of-long-videos/

<img src='/imgs/Flexible Diffusion Modeling of Long Videos.gif'>

---

## Other workds

🌐 [**Map Track – CARLA**](https://leaderboard.carla.org/leaderboard/)  

🌐 [**paperswithcode.com: Bird's-Eye View Semantic Segmentation**](https://paperswithcode.com/task/bird-s-eye-view-semantic-segmentation/codeless)  

📄 [**awesome-lane-detection**](https://github.com/amusi/awesome-lane-detection)

📄 [**TransformerFusion: Monocular RGB SceneReconstruction using Transformers**](https://arxiv.org/abs/2107.02191)  
📺 https://www.youtube.com/watch?v=ys-LRewgNYs  

📄 [**Calibration of Inverse Perspective Mapping from Different Road Surface Images**](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9612531)

