# ðŸ“– Deep Learning for Satellite and Street View Mapping: A Survey

**Driving among Flatmobiles: Bird-Eye-View occupancy grids from a monocular camera for holistic trajectory planning**

|<img src='/imgs/Driving among Flatmobiles.jpg'>
|:--:|
| Figure 6: Bird-eye-view qualitative results for the first stage of the network. The blue part of the predicted masks corresponds to the limits of the cameraâ€™s field of view. GT stands for Ground Truth. |

Paper: https://arxiv.org/abs/2008.04047  
Video: https://www.youtube.com/watch?v=ys-LRewgNYs  


**Monocular Semantic Occupancy Grid Mapping with Convolutional Variational Encoder-Decoder Networks**

|<img src='/imgs/Monocular Semantic Occupancy.jpg'>
|:--:|
| Fig. 1. An illustration of the proposed variational encoder-decoder approach. From a single front-view RGB image, our system can predict a 2-D top-view semantic-metric occupancy grid map. |

Paper: https://arxiv.org/abs/1804.02176


**Orthographic Feature Transform for Monocular 3D Object Detection**

|<img src='/imgs/Orthographic Feature Transform.jpg'>
|:--:|
| Figure 3. Architecture overview. A front-end ResNet feature extractor generates image-based features, which are mapped to an orthographic representation via our proposed orthographic feature transform. The topdown network processes these features in the birds-eye-view space and at each location on the ground plane predicts a confidence score S, a position offset âˆ†pos, a dimension offset âˆ†dim and an angle vector âˆ†ang. |

Paper: https://arxiv.org/abs/1811.08188

