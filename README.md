# ğŸ“– Deep Learning for Satellite and Street View Mapping: A Survey

---

ğŸ“„ [**Cross-view Transformers for real-time Map-view Semantic Segmentation**](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Cross-View_Transformers_for_Real-Time_Map-View_Semantic_Segmentation_CVPR_2022_paper.pdf)  
ğŸŒ https://github.com/bradyz/cross_view_transformers

<img src='https://raw.githubusercontent.com/bradyz/cross_view_transformers/master/docs/assets/predictions.gif' width="50%">
<img src='https://raw.githubusercontent.com/bradyz/cross_view_transformers/master/docs/assets/map.gif' width="50%">

---

ğŸ“„ [**NEAT: Neural Attention Fields for End-to-End Autonomous Driving**](https://openaccess.thecvf.com/content/ICCV2021/html/Chitta_NEAT_Neural_Attention_Fields_for_End-to-End_Autonomous_Driving_ICCV_2021_paper.html)  
ğŸŒ https://github.com/autonomousvision/neat  
ğŸ“º https://youtu.be/gtO-ghjKkRs

<img src='https://github.com/autonomousvision/neat/raw/main/neat/assets/neat_clip.GIF'>

---

ğŸ“„ [**Driving among Flatmobiles: Bird-Eye-View occupancy grids from a monocular camera for holistic trajectory planning**](https://arxiv.org/abs/2008.04047)  
ğŸ“º https://www.youtube.com/watch?v=ys-LRewgNYs  

|<img src='/imgs/Driving among Flatmobiles.jpg'>
|:--:|
| Figure 6: Bird-eye-view qualitative results for the first stage of the network. The blue part of the predicted masks corresponds to the limits of the cameraâ€™s field of view. GT stands for Ground Truth. |![image](https://user-images.githubusercontent.com/22514465/180946369-a6d475d1-2a21-43e2-bcb5-4445f4e98eab.jpeg)


---

ğŸ“„ [**Monocular Semantic Occupancy Grid Mapping with Convolutional Variational Encoder-Decoder Networks**](https://arxiv.org/abs/1804.02176)

|<img src='/imgs/Monocular Semantic Occupancy.jpg'>
|:--:|
| Fig. 1. An illustration of the proposed variational encoder-decoder approach. From a single front-view RGB image, our system can predict a 2-D top-view semantic-metric occupancy grid map. |

---

ğŸ“„ [**Orthographic Feature Transform for Monocular 3D Object Detection**](https://arxiv.org/abs/1811.08188)

|<img src='/imgs/Orthographic Feature Transform.jpg'>
|:--:|
| Figure 3. Architecture overview. A front-end ResNet feature extractor generates image-based features, which are mapped to an orthographic representation via our proposed orthographic feature transform. The topdown network processes these features in the birds-eye-view space and at each location on the ground plane predicts a confidence score S, a position offset âˆ†pos, a dimension offset âˆ†dim and an angle vector âˆ†ang. |

---

ğŸ“„ [**Monocular 3D Vehicle Detection Using Uncalibrated Traffic Cameras through Homography**](https://arxiv.org/abs/2103.15293)  
ğŸŒ https://github.com/minghanz/trafcam_3d

|<img src='/imgs/Monocular 3D Vehicle Detection Using Uncalibrated Traffic Cameras through Homography.jpg'>
|:--:|
| Fig. 1: The 3D vehicle detection problem is transformed to a 2D detection problem in warped birdâ€™s eye view (BEV) images. The orange lines attached to each orange boxes are tails, defined in Sec. III-C.1 and Fig. 5, which are regressed by the network to better handle distortions in BEV images. |

---

ğŸ“„ [**Unsupervised Learning of Depth and Ego-Motion from Video**](https://paperswithcode.com/paper/unsupervised-learning-of-depth-and-ego-motion-1)
ğŸŒ https://github.com/tinghuiz/SfMLearner
ğŸ“º https://youtu.be/HWu39YkGKvI

<img src='https://github.com/tinghuiz/SfMLearner/raw/master/misc/cityscapes_sample_results.gif'>

---

ğŸ“„ [**Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations**](https://arxiv.org/abs/2111.13152)  
ğŸŒ https://srt-paper.github.io

<img src='https://srt-paper.github.io/data/streetview/input1.png'>

---

ğŸ“„ [**Look Outside the Room: Synthesizing A Consistent Long-Term 3D Scene Video from A Single Image**](https://arxiv.org/abs/2203.09457)  
ğŸŒ https://xrenaa.github.io/look-outside-room/  
ğŸ“º https://youtu.be/eSftXilv21s  

<img src='https://xrenaa.github.io/look-outside-room/static/images/method.png' width="70%">

---

ğŸ“„ [**MP3: A Unified Model to Map, Perceive, Predict and Plan**](https://arxiv.org/abs/2101.06806)  

|<img src='/imgs/MP3 - A Unified Model to Map, Perceive, Predict and Plan.jpg'>
|:--:|
| Figure 2: MP3 predicts probabilistic scene representations that are leveraged in motion planning as interpretable cost functions. |

---

ğŸ“„ [**Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D**](https://arxiv.org/abs/2008.05711)  
ğŸŒ https://nv-tlabs.github.io/lift-splat-shoot/  
ğŸŒ https://github.com/nv-tlabs/lift-splat-shoot  
ğŸ“º https://youtu.be/oL5ISk6BnDE

<img src='/imgs/Lift, Splat, Shoot.gif'>

---

## Other papers

ğŸ“„ [**awesome-lane-detection**](https://github.com/amusi/awesome-lane-detection)

ğŸ“„ [**TransformerFusion: Monocular RGB SceneReconstruction using Transformers**](https://arxiv.org/abs/2107.02191)  
ğŸ“º https://www.youtube.com/watch?v=ys-LRewgNYs  

ğŸ“„ [**Calibration of Inverse Perspective Mapping from Different Road Surface Images**](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9612531)

